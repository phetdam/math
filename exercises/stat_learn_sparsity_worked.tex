\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, enumitem, fancyhdr, tikz}
% get rid of paragraph indent
\setlength{\parindent}{0 pt}
% allow section.equation numbering
\numberwithin{equation}{section}
\usepackage{hyperref}
% make the link colors blue, as well as cite colors. urls are magenta
\hypersetup{colorlinks, linkcolor=blue, citecolor=blue, urlcolor=magenta}

\title{Statistical Learning with Sparsity \\ \Large Worked Exercises}
\author{Derek Huang\thanks{SMBC Capital Markets, Quantitative Strategists.}}
\date{March 22, 2022}

\begin{document}

\newcommand{\newtocsubsection}[1]{%
    \subsection*{#1} \addcontentsline{toc}{subsection}{#1}%
}

\maketitle

%\newpage

\tableofcontents

\newpage

\section{Introduction}

During my time as an undergraduate student at NYU, there was a period where
I found it interesting how the theory of convex optimization was applied by
the field of supervised learning to fit models. One of the topics I was
interested in was sparse optimization, mostly the kinds where sparsity was
enforced by a convex norm constraint, ex. $ \ell^1 $ norm for vectors,
nuclear norm for matrices.

\medskip

To satisfy my interests, I ended up purchasing Hastie, Tibshirani, and
Wainwright's \textit{%
    Statistical Learning with Sparsity: The Lasso and Generalizations%
}, seeing that it was a monograph on the topic. Despite enjoying the portions
of the book that I found time to read, I was irrationally afraid of
attempting the exercises, mostly because of a subconscious belief that failure
to satisfactorily complete meaningfully challenging exercises would somehow
invalidate both my intelligence and my worth as an individual. I no longer
feel the same way, although I still think that in general, my own intelligence
and individual worth are nothing significant.

\medskip

Regardless, the worked exercises in this document exist to be a testament
to my own efforts towards understanding and will hopefully be a resource to
anyone else who may be attempting these exercises.

\medskip

\fbox{%
    \parbox{\textwidth}{%
        \textbf{Disclaimer:} This is \textcolor{red}{\textbf{not}} an official
        solution guide for the text. I \textbf{strongly} \textbf{recommend}
        that one attempt the text's exercises \textbf{on} \textbf{their}
        \textbf{own} before consulting this document, as I believe active
        self-learning truly plays an outsized role in determining the depth of
        one's understanding. One's instructors are there to guide and support,
        but all must walk their paths to understanding themselves.%
    }%
}

\section{The Lasso for Linear Models}

\newtocsubsection{Exercise 2.1} \label{sec:2.1}

Let $ \mathbf{x}_j  \in \mathbb{R}^N $ denote the $ j $-th column of the
predictor matrix $ \mathbf{X} \in \mathbb{R}^{N \times d} $, and let
$ \mathbf{y} \in \mathbb{R}^N $ denote the response vector as usual. Under
the assumptions of section 2.4, we have $ \frac{1}{N}\mathbf{1}^\top
\mathbf{y} = 0 $, $ \frac{1}{N}\mathbf{1}^\top\mathbf{x}_j = 0 $,
$ \frac{1}{N}\mathbf{x}_j^\top\mathbf{x}_j = 1 $\footnote{
    In words, the predictors are standardized to zero mean and unit variance,
    while the response is standardized to zero mean.
}.

\medskip

The lasso problem, as introduced in the book, can be written as
\begin{equation} \label{eq:std_lasso}
    \min_\mathbf{w}\frac{1}{2N}\Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2 +
    \lambda\Vert\mathbf{w}\Vert_1
\end{equation}

Let us replace $ \lambda \in (0, \infty) $ with $ \lambda_j \in
(0, \infty) $, where $ \forall j \in \{1, \ldots d\} $, $ \lambda_j
\Rightarrow w_{j'} \in \{1, \ldots d\}, j' \ne j, w_{j'} = 0 $.
With $ \lambda \triangleq \lambda_j $, only $ w_j $ may be nonzero, and
so in that case we can rewrite (\ref{eq:std_lasso}) where
$ \lambda \triangleq \lambda_j $ as
\begin{equation*}
    \min_{w_j}\frac{1}{2N}\sum_{k = 1}^N(y_k - x_{kj}w_j)^2 +
    \lambda_j|w_j|
\end{equation*}

Taking the subderivative of the above expression at its minimizer
$ \hat{w}_j $, we have
\begin{equation*}
    0 \in \left\{
        -\frac{1}{N}\sum_{k = 1}^Nx_{kj}(y_k - x_{kj}\hat{w}_j)
    \right\} +
    \lambda_j\partial|\hat{w}_j| =
    \left\{-\frac{1}{N}\mathbf{x}_j^\top\mathbf{y} + \hat{w}_j\right\} +
    \lambda_j\partial|\hat{w}_j|
\end{equation*}

Note that $ \hat{w}_j = 0 \Rightarrow \partial|\hat{w}_j| = [-1, 1] $. We
must then have $ \left|\frac{1}{N}\mathbf{x}_j^\top\mathbf{y}\right| \le
\lambda_j \Rightarrow \left|\frac{1}{N}\mathbf{x}_j^\top\mathbf{y}\right| $
is the smallest value of $ \lambda $ that guarantees $ \hat{w}_j = 0 $
conditional on $ \hat{w}_{j'} = 0 $, $ \forall j' \in \{1, \ldots d\} $,
$ j' \ne j $. Therefore, the smallest value of $ \lambda $,
which we denote $ \tilde{\lambda} $, that unconditionally guarantees
$ \hat{\mathbf{w}} = \mathbf{0} $, is such that
\begin{equation*}
    \tilde{\lambda} \triangleq
    \frac{1}{N}\max\big\{
        \big|\mathbf{x}_1^\top\mathbf{y}\big|, \ldots
        \big|\mathbf{x}_d^\top\mathbf{y}\big|
    \big\}
\end{equation*}

\newtocsubsection{Exercise 2.2}

The scalar soft-thresholding operator $ \mathcal{S}_\lambda $ presented in
section 2.4.1 is defined such that for $ \lambda \in (0, \infty) $,
\begin{equation} \label{eq:soft_threshold}
    \mathcal{S}_\lambda(x) \triangleq
    \operatorname{sgn}(x)\max\{0, |x| - \lambda\}
\end{equation}

For a scalar predictor, where we have an input vector $ \mathbf{x} \in
\mathbb{R}^N $, the lasso problem (\ref{eq:std_lasso}) simplifies to
\begin{equation} \label{eq:single_lasso}
    \min_w\frac{1}{2N}\sum_{k = 1}^N(y_k - x_kw)^2 + \lambda|w|
\end{equation}

We could solve for the minimizer $ \hat{w} \in \mathbb{R} $ of
(\ref{eq:single_lasso}) by taking a subderivative, but we are explicitly
told not to, so we instead proceed by considering the different cases of
$ \hat{w} $. Naturally, for simplicity, we first suppose that
$ \hat{w} \in (0, \infty) $. In that case, we can take a derivative of
(\ref{eq:single_lasso}) at $ \hat{w} $, and thus have
\begin{equation*}
    0 = -\frac{1}{N}\sum_{k = 1}^Nx_k(y_k - x_k\hat{w}) + \lambda =
    -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} + \lambda \Leftrightarrow
    \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} - \lambda
\end{equation*}

Here we use the fact that as in \nameref{sec:2.1}, the input vector
$ \mathbf{x} $ is given standardized, where $ \frac{1}{N}\mathbf{1}^\top
\mathbf{x} = 0 $, $ \frac{1}{N}\mathbf{x}^\top\mathbf{x} = 1 $, so
$ \frac{1}{N}\sum_{k = 1}^Nx_k^2\hat{w} $ simplifies to $ \hat{w} $. Now,
considering the case when $ \hat{w} \in (-\infty, 0) $, we have
\begin{equation*}
    0 = -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} - \lambda
    \Leftrightarrow \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} + \lambda
\end{equation*}

Noting that $ \lambda \in (0, \infty) $, we thus have $ \hat{w} \in
(0, \infty) \Leftrightarrow \frac{1}{N}\mathbf{x}^\top\mathbf{y} > \lambda $,
$ \hat{w} \in (-\infty, 0) \Leftrightarrow \frac{1}{N}\mathbf{x}^\top
\mathbf{y} < -\lambda $, which may be simplified into the condition that
$ \hat{w} \ne 0 \Leftrightarrow \left|\frac{1}{N}\mathbf{x}^\top
\mathbf{y}\right| > \lambda $. Therefore, by \textit{modus tollens} in both
directions, $ \hat{w} = 0 \Leftrightarrow \left|\frac{1}{N}\mathbf{x}^\top
\mathbf{y}\right| \le \lambda $. Furthermore, we can express $ \hat{w} $ in
terms of $ \frac{1}{N}\mathbf{x}^\top\mathbf{y} $ such that
\begin{equation*}
    \hat{w} =
    \operatorname{sgn}\left(\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right)
    \max\left\{
        \left|\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right| - \lambda, 0
    \right\} \triangleq
    \mathcal{S}_\lambda\left(\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right)
\end{equation*}

(\ref{eq:soft_threshold}) therefore yields the solution to
(\ref{eq:single_lasso}) when applied to
$ \frac{1}{N}\mathbf{x}^\top\mathbf{y} $.

\newtocsubsection{Exercise 2.3}

The subgradient, more precisely subderivative, equation for
(\ref{eq:single_lasso}) at a minimizer $ \hat{w} \in \mathbb{R} $ is
\begin{equation*}
    0 \in
    \left\{-\frac{1}{N}\sum_{k = 1}^Nx_k(y_k - x_k\hat{w})\right\} +
    \lambda\partial|\hat{w}| \Rightarrow
    0 \in
    \left\{-\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w}\right\} +
    \lambda\partial|\hat{w}|
\end{equation*}

Here we use the fact that $ \mathbf{x} \in \mathbb{R}^N $ is standardized
with $ \frac{1}{N}\mathbf{1}^\top\mathbf{x} = 0 $,
$ \frac{1}{N}\mathbf{x}^\top\mathbf{x} = 1 $. By the standard theory of
subgradients and considering the cases where $ \hat{w} \in (0, \infty) $,
$ \hat{w} \in(-\infty, 0) $, $ \hat{w} = 0 $, we see that
\begin{equation*}
    \begin{split}
            \hat{w} \in (0, \infty) & \Leftrightarrow
            0 \in \left\{
                -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} + \lambda
            \right\} \Leftrightarrow
            \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} - \lambda
            \Leftrightarrow \frac{1}{N}\mathbf{x}^\top\mathbf{y} > \lambda \\
            \hat{w} \in (-\infty, 0) & \Leftrightarrow
            0 \in \left\{
                -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} - \lambda
            \right\} \Leftrightarrow
            \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} + \lambda
            \Leftrightarrow \frac{1}{N}\mathbf{x}^\top\mathbf{y} < -\lambda \\
            \hat{w} = 0 & \Leftrightarrow
            0 \in \left\{-\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right\} +
            \lambda[-1, 1] \Leftrightarrow
            \left|-\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right| \le \lambda
    \end{split}
\end{equation*}

We can write our results more neatly in the style of the book as
\begin{equation} \label{eq:2.3.1}
    \hat{w} = \left\{
        \begin{aligned}
            & \frac{1}{N}\mathbf{x}^\top\mathbf{y} - \lambda,
            & & \frac{1}{N}\mathbf{x}^\top\mathbf{y} > \lambda \\
            & 0, &
            & \frac{1}{N}\left|\mathbf{x}^\top\mathbf{y}\right| \le \lambda \\
            & \frac{1}{N}\mathbf{x}^\top\mathbf{y} + \lambda,
            & & \frac{1}{N}\mathbf{x}^\top\mathbf{y} < -\lambda
        \end{aligned}
    \right.
\end{equation}

Finally, from (\ref{eq:soft_threshold}), it's clear that we can express
(\ref{eq:2.3.1}) more succinctly with $ \mathcal{S}_\lambda $ as
\begin{equation*}
    \hat{w} =
    \mathcal{S}_\lambda\left(\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right)
\end{equation*}

\newtocsubsection{Exercise 2.4}

The subgradient equation for (\ref{eq:std_lasso}) at a minimizer
$ \mathbf{\hat{w}} \in \mathbb{R}^d $ in vector-matrix notation is
\begin{equation*}
    \mathbf{0} \in \left\{
        -\frac{1}{N}\mathbf{X}^\top(
            \mathbf{y} - \mathbf{X}\hat{\mathbf{w}}
        )
    \right\} +
    \lambda\partial\Vert\hat{\mathbf{w}}\Vert_1
\end{equation*}

We can also write the subderivative equation for each predictor $ j \in
\{1, \ldots d\} $, which is
\begin{equation} \label{eq:2.4.1}
    0 \in \left\{
        -\frac{1}{N}\mathbf{x}_j^\top(\mathbf{y} - \mathbf{X}\hat{\mathbf{w}})
    \right\} +
    \lambda\partial|\hat{w}_j|
\end{equation}

From (\ref{eq:2.4.1}), we can derive the update applied to $ \hat{w}_j $ at
each iteration of coordinate descent, as

\medskip

TODO

\newtocsubsection{Exercise 2.7}

Let us consider the [random] response vector $ \mathbf{y} $, where
$ \mathbf{y}(\Omega) = \mathbb{R}^N $. Suppose we model $ \mathbf{y} $
using a non-adaptive linear model with independent, finite variance
additive errors. Then,
\begin{equation} \label{eq:std_linear_model}
    \mathbf{y} \triangleq \mathbf{Xw} + \varepsilon
\end{equation}

Here $ \mathbf{X} $ is the [random] input matrix, assumed
centered\footnote{
    We omit the intercept $ b $, since if the inputs are centered, for
    fixed $ \mathbf{y} $, $ \mathbf{X} $, its estimate $ \hat{b} \triangleq
    \frac{1}{N}\big(\mathbf{1}^\top\mathbf{y} -
    \mathbf{1}^\top\mathbf{X}\hat{\mathbf{w}}\big) $.
}, where $ \mathbf{X}(\Omega) = \mathbb{R}^{N \times d} $, $ \mathbf{w} \in
\mathbb{R}^d $ the true parameter vector, and $ \varepsilon $ the error
vector with i.i.d. components, where $ \forall k \in\{1, \ldots N\} $,
$ \mathbb{E}[\varepsilon_k] = 0 $, $ \operatorname{Var}(\varepsilon_k) =
\sigma^2 $. To fit (\ref{eq:std_linear_model}) by least squares, we want to
solve the convex problem
\begin{equation} \label{eq:std_linear_lsq}
    \min_\mathbf{w}\Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2
\end{equation}

Let us assume that our $ d < N $ predictors are not collinear\footnote{
    If $ \operatorname{rank}(\mathbf{X}) \le \min\{N, d\} $, it is easy to
    see from our solution that the model's degrees of freedom will be
    $ \operatorname{rank}(\mathbf{X}) $.
}, i.e. $ \operatorname{rank}(\mathbf{X}) = d $. To simplify our analysis,
by [compact] singular value decomposition, we can write $ \mathbf{X} =
\mathbf{U\Sigma V}^\top $, where $ \mathbf{U} $, $ \mathbf{V} $ have $ d $
orthonormal columns and $ \mathbf{\Sigma} $ is $ d \times d $ with positive
diagonal. Since the minimizer $ \hat{\mathbf{w}} $ of
(\ref{eq:std_linear_lsq}) is such that $ \mathbf{X}^\top\mathbf{X}
\hat{\mathbf{w}} = \mathbf{X}^\top\mathbf{y} $, then
\begin{equation*}
    \mathbf{X}^\top\mathbf{X}\hat{\mathbf{w}} =
    \mathbf{V\Sigma}^2\mathbf{V}^\top\hat{\mathbf{w}} =
    \mathbf{V\Sigma U}^\top\mathbf{y} \Rightarrow
    \mathbf{V}^\top\hat{\mathbf{w}} =
    \mathbf{\Sigma}^{-1}\mathbf{U}^\top\mathbf{y}
\end{equation*}

Since the [random] predicted values $ \hat{\mathbf{y}} $ are such that
$ \hat{\mathbf{y}} \triangleq \mathbf{X}\hat{\mathbf{w}} $, we can use the
above result to write $ \hat{\mathbf{y}} $ in terms of $ \varepsilon $,
since with $ \mathbf{X} $ fixed $ \varepsilon $ becomes the only source of
randomness. We therefore see that
\begin{equation} \label{eq:2.7.1}
    \hat{\mathbf{y}} =
    \mathbf{U\Sigma}\big(\mathbf{V}^\top\hat{\mathbf{w}}\big) =
    \mathbf{UU}^\top\big(
        \mathbf{U\Sigma V}^\top\mathbf{w} + \varepsilon
    \big) = \mathbf{Xw} + \mathbf{UU}^\top\varepsilon
\end{equation}

As given in the text, the degrees of freedom $ \operatorname{df}(
\hat{\mathbf{y}}) $ of the specified linear model is such that
\begin{equation*}
    \operatorname{df}(\hat{\mathbf{y}}) \triangleq
    \frac{1}{\sigma^2}\sum_{k = 1}^N
    \operatorname{cov}(\hat{y}_k, y_k \mid \mathbf{X})
\end{equation*}

Since $ \operatorname{cov}(\hat{y}_k, y_k \mid \mathbf{X}) =
\mathbb{E}[\hat{y}_ky_k \mid \mathbf{X}] -
\mathbb{E}[\hat{y}_k \mid \mathbf{X}]\mathbb{E}[y_k \mid \mathbf{X}] $, by
linearity of expectation and by (\ref{eq:2.7.1}) we have
\begin{equation} \label{eq:2.7.2}
    \operatorname{df}(\hat{\mathbf{y}}) =
    \frac{1}{\sigma^2}
    \mathbb{E}\big[\hat{\mathbf{y}}^\top\mathbf{y} \ \big| \ \mathbf{X}\big] -
	\frac{1}{\sigma^2}\mathbb{E}[\hat{\mathbf{y}} \mid \mathbf{X}]^\top
    \mathbb{E}[\mathbf{y} \mid \mathbf{X}]
\end{equation}

Expanding the first and second terms of (\ref{eq:2.7.2}) and noting
$ \mathbb{E}[\varepsilon \mid \mathbf{X}] = \mathbf{0} $, we have from
(\ref{eq:std_linear_model}) and (\ref{eq:2.7.1})
\begin{equation*}
    \begin{split}
        \mathbb{E}\big[
            \hat{\mathbf{y}}^\top\mathbf{y} \ \big| \ \mathbf{X}
        \big] & =
        \mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} +
        \mathbf{w}^\top\mathbf{X}^\top
        \mathbb{E}[\varepsilon \mid \mathbf{X}] +
        \mathbb{E}[\varepsilon \mid \mathbf{X}]^\top
        \mathbf{UU}^\top\mathbf{Xw} +
        \mathbb{E}\big[
            \varepsilon^\top\mathbf{UU}^\top\varepsilon \ \big| \
            \mathbf{X}
        \big] \\ & =
        \mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} +
        \mathbb{E}\big[
            \varepsilon^\top\mathbf{UU}^\top\varepsilon \ \big| \
            \mathbf{X}
        \big] \\
        \mathbb{E}[\hat{\mathbf{y}} \mid \mathbf{X}]^\top
        \mathbb{E}[\mathbf{y} \mid \mathbf{X}] & =
        \mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw}
    \end{split}
\end{equation*}

We thus see that $ \operatorname{df}(\hat{\mathbf{y}}) = \frac{1}{\sigma^2}
\mathbb{E}\big[\varepsilon^\top\mathbf{UU}^\top\varepsilon \ \big| \
\mathbf{X}\big] $. Letting $ \mathbf{u}_j $ denote the $ j $th column of
$ \mathbf{U} $, then
\begin{equation*}
    \operatorname{df}(\hat{\mathbf{y}}) =
    \frac{1}{\sigma^2}\mathbb{E}\left[
        \begin{bmatrix}
            \ \varepsilon^\top\mathbf{u}_1 &
            \ldots &
            \varepsilon^\top\mathbf{u}_d \
        \end{bmatrix}
        \begin{bmatrix}
            \ \mathbf{u}_1^\top\varepsilon \ \\
            \ \vdots \ \\
            \ \mathbf{u}_d^\top\varepsilon \
        \end{bmatrix}
        \ \middle\vert \ \mathbf{X}
    \right] =
    \frac{1}{\sigma^2}\mathbb{E}\left[
        \sum_{j = 1}^d\big(\mathbf{u}_j^\top\varepsilon\big)^2
        \ \middle\vert \ \mathbf{X}
    \right] =
    \frac{1}{\sigma^2}\sum_{j = 1}^d\mathbb{E}\left[
        \big(\mathbf{u}_j^\top\varepsilon\big)^2
        \ \middle\vert \ \mathbf{X}
    \right]
\end{equation*}

Since $ \mathbb{E}\big[\mathbf{u}_j^\top\varepsilon \ \big| \
\mathbf{X}\big] = 0 $, then $ \mathbb{E}\left[\big(\mathbf{u}_j^\top
\varepsilon\big)^2 \ \middle\vert \ \mathbf{X}\right] = \operatorname{Var}
\big(\mathbf{u}_j^\top\varepsilon \ \big| \ \mathbf{X}\big) $. Recalling
that each $ \varepsilon_1, \ldots \varepsilon_N $ is i.i.d. with
$ \mathbb{E}[\varepsilon_k] = 0 $, $ \operatorname{Var}(\varepsilon_k) =
\sigma^2 $, $ \varepsilon \perp \mathbf{X} $, we see that
$ \operatorname{Var}\big(\mathbf{u}_j^\top\varepsilon \ \big| \
\mathbf{X}\big) = \sum_{k = 1}^Nu_{jk}^2\operatorname{Var}(\varepsilon_k) =
\sigma^2\mathbf{u}_j^\top\mathbf{u}_j = \sigma^2 $. Note that
$ \mathbf{u}_j^\top\mathbf{u}_j = 1 $ by the definition of the orthonormal
vector. Therefore, we see that
\begin{equation*}
    \operatorname{df}(\hat{\mathbf{y}}) =
    \frac{1}{\sigma^2}\sum_{j = 1}^d\operatorname{Var}\big(
        \mathbf{u}_j^\top\varepsilon \ \big| \ \mathbf{X}
    \big) = d
\end{equation*}

Therefore, a linear model fit by least squares using $ d $ non-collinear
predictors has exactly $ d $ degrees of freedom.

\section{Generalized Linear Models}

\newtocsubsection{Exercise 3.1}

\fbox{%
    \parbox{\textwidth}{%
        \textbf{Note:} I believe the authors made a semantic error in the
        problem's wording, as it should be the parameter estimates that
        are, or rather tend towards being, infinite, not the likelihood
        estimates themselves, as those are constrained within $ (0, 1) $
        given any $ \mathbf{w} \in \mathbb{R}^d $. Also, to be more precise,
        the optimal value of the unregularized objective is reached only in
        the limit, and is never achieved by any finite $ \mathbf{w} $.
    }%
}

\medskip

Consider training data pairs $ \mathcal{D} \triangleq
\{(\mathbf{x}_1, y_1), \ldots (\mathbf{x}_N, y_N)\} $, where $ \forall k \in
\{1, \ldots N\} $, $ \mathbf{x}_k \in \mathbb{R}^d $, $ y_k \in \{-1, 1\} $.
We choose the symmetric labeling scheme since it results in a more elegant
objective without loss of generality and will discuss the case where
$ y_k \in \{0, 1\} $ afterwards. Since the two labeled classes of points are
linearly separable, $ \exists \tilde{\mathbf{w}} \in \mathbb{R}^d $,
$ \tilde{b} \in \mathbb{R} $, $ \tilde{f}(\mathbf{x}) \triangleq
\tilde{\mathbf{w}}^\top\mathbf{x} + \tilde{b} $ s.t.
$ \forall (\mathbf{x}, y) \in \mathcal{D} $, $ y\tilde{f}(\mathbf{x}) > 0
\Rightarrow y = \operatorname{sgn} \circ \tilde{f}(\mathbf{x}) $. In words,
the margin is always positive $ \Rightarrow \tilde{f} $ perfectly classifies
each of the data points.

\medskip

Under the $ \{-1, 1\} $ labeling scheme, the [scaled] negative log-likelihood
for two-class logistic regression is
\begin{equation} \label{eq:logreg_binary_mle}
    \min_{\mathbf{w}, b}\frac{1}{N}\sum_{k = 1}^N\log\left(
        1 + e^{-y_k\left(\mathbf{w}^\top\mathbf{x}_k + b\right)}
    \right)
\end{equation}

Consider some $ \varepsilon \in (0, \infty) $. Since
$ \forall (\mathbf{x}, y) \in \mathcal{D} $, $ y\tilde{f}(\mathbf{x}) > 0 $,
obviously $ \varepsilon y\tilde{f}(\mathbf{x}) > 0 \Rightarrow
\varepsilon\tilde{\mathbf{w}}, \varepsilon\tilde{b} $ also results in perfect
linear separability of the two classes. However, note that
\begin{equation*}
    \lim_{\varepsilon \rightarrow \infty}\frac{1}{N}\sum_{k = 1}^N\log\left(
        1 + e^{-\varepsilon y_k\tilde{f}(\mathbf{x}_k)}
    \right) =
    \frac{1}{N}\sum_{k = 1}^N\log\left(
        1 + \lim_{\varepsilon \rightarrow \infty}
        e^{-\varepsilon y_k\tilde{f}(\mathbf{x}_k)}
    \right) =
    \log 1 = 0
\end{equation*}

Here we use the fact that continuous functions respect limits and that
$ \forall a \in (0, \infty) $,
$ \lim_{\varepsilon \rightarrow \infty}e^{-a\varepsilon} = 0 $. Clearly,
(\ref{eq:logreg_binary_mle}) approaches its optimal value of zero in the
limit as the norms of positive multiples of $ \tilde{\mathbf{w}} $,
$ \tilde{b} $ approach infinity. Furthermore, we can show that the fitted
probabilities $ \tilde{\mathbb{P}}_\varepsilon\{Y = 1 \mid X = \mathbf{x}\} $,
$ \tilde{\mathbb{P}}_\varepsilon\{Y = -1 \mid X = \mathbf{x}\} $ are
well-defined. Let $ \tilde{\mathbb{P}}_\varepsilon
\{Y = y \mid X = \mathbf{x}\} $ be defined such that
\begin{equation} \label{eq:3.1.1}
    \tilde{\mathbb{P}}_\varepsilon\{Y = y \mid X = \mathbf{x}\} \triangleq
    \frac{1}{1 + e^{-\varepsilon y\tilde{f}(\mathbf{x})}} \triangleq
    \sigma\circ\varepsilon y\tilde{f}(\mathbf{x})
\end{equation}

Here $ \sigma : \mathbb{R} \rightarrow (0, 1) $ is the sigmoid function,
defined such that $ \sigma(z) \triangleq \frac{1}{1 + e^{-z}} $. Note that
(\ref{eq:3.1.1}) results from simple rearrangement of the standard linear
model for the log of the likelihood ratio, i.e.
\begin{equation*}
    \log\frac{
        \tilde{\mathbb{P}}_\varepsilon\{Y = 1 \mid X = \mathbf{x}\}
    }{
        \tilde{\mathbb{P}}_\varepsilon\{Y = -1 \mid X = \mathbf{x}\}
    } =
    \varepsilon\tilde{f}(\mathbf{x})
\end{equation*}

We note that after solving for $ \tilde{\mathbb{P}}_\varepsilon
\{Y = 1 \mid X = \mathbf{x}\} $, we can see that
\begin{equation*}
    \begin{split}
        \tilde{\mathbb{P}}_\varepsilon\{Y = 1 \mid X = \mathbf{x}\} & =
        \frac{
            e^{\varepsilon\tilde{f}(\mathbf{x})}
        }{
            1 + e^{\varepsilon\tilde{f}(\mathbf{x})}
        } =
        \frac{1}{1 + e^{-\varepsilon\tilde{f}(\mathbf{x})}} \triangleq
        \sigma\circ\varepsilon\tilde{f}(\mathbf{x}) \\
        \tilde{\mathbb{P}}_\varepsilon\{Y = -1 \mid X = \mathbf{x}\} & =
        1 - \tilde{\mathbb{P}}_\varepsilon\{Y = 1 \mid X = \mathbf{x}\} =
        \frac{1}{1 + e^{\varepsilon\tilde{f}(\mathbf{x})}} \triangleq
        \sigma\circ -\varepsilon\tilde{f}(\mathbf{x})
    \end{split}
\end{equation*}

For fixed $ y \in \{-1, 1\} $ and $ \forall \varepsilon \in (0, \infty) $,
$ \tilde{\mathbb{P}}_\varepsilon\{Y = y \mid X = \mathbf{x}\} $ as defined
in (\ref{eq:3.1.1}) is a well-defined function of $ \mathbf{x} $, as it is
just a composition of a bijective\footnote{
    $ \exists\sigma^{-1} : (0, 1) \rightarrow \mathbb{R} $ where
    $ \sigma^{-1}(z) \triangleq -\log\left(z^{-1} - 1\right) $.
}, strictly monotone increasing\footnote{
    $ \exists\sigma': \mathbb{R} \rightarrow (0, 1) $ where
    $ \sigma'(z) \triangleq \frac{e^{-z}}{(1 + e^{-z})^2} =
    \frac{1}{1 + e^{-z}}\left(\frac{e^{-z}}{1 + e^{-z}}\right) \triangleq
    \sigma(z)(1 - \sigma(z)) $. Note $ (-\infty, 0] \cap
    \operatorname{im}\sigma' = \emptyset $.
}
function with an affine function. Clearly, $ \operatorname{dom}\varepsilon
y\tilde{f} = \mathbb{R}^d $, $ \operatorname{im}\varepsilon
y\tilde{f} \subseteq \mathbb{R} $, so $ \sigma\circ\varepsilon y\tilde{f} $
is itself well-defined as a function. It is also clear that from inspecting
(\ref{eq:3.1.1}) that $ \lim_{\varepsilon \rightarrow \infty}
\tilde{\mathbb{P}}_\varepsilon\{Y = y \mid X = \mathbf{x}\} =
\mathbb{I}_{(0, \infty)}\circ y\tilde{f}(\mathbf{x}) $, i.e. a step function
of the margin.

\medskip

TODO: quick note on $ \{0, 1\} $ labeling case

\newtocsubsection{Exercise 3.2}

Consider an input point and label pair $ (\mathbf{x}, y) \in \mathbb{R}^d
\times \{-1, 1\} $, a linear classification function
$ f : \mathbb{R}^d \rightarrow \mathbb{R} $ where
$ f(\mathbf{x}) \triangleq \mathbf{w}^\top\mathbf{x} + b $, and its
corresponding decision boundary $ \mathcal{B} \triangleq \{\mathbf{x}' \in
\mathbb{R}^d : f(\mathbf{x}') = 0\} $. Note that $ \forall \mathbf{x}',
\tilde{\mathbf{x}} \in \mathcal{B} $, $ f(\mathbf{x}') -
f(\tilde{\mathbf{x}}) = \mathbf{w}^\top(\mathbf{x}' - \tilde{\mathbf{x}}) =
0 \Rightarrow \mathbf{w} \perp \mathbf{x}' - \tilde{\mathbf{x}} $, i.e.
$ \mathbf{w} $, and thus $ \mathbf{w} / \Vert\mathbf{w}\Vert_2 $, are
normal to $ \mathcal{B} $.

\medskip

Furthermore, $ \forall \tilde{\mathbf{x}} \in \mathcal{B} $,
$ \mathbf{w}^\top(\mathbf{x} - \tilde{\mathbf{x}}) / \Vert\mathbf{w}\Vert_2 =
\Vert\mathbf{x} - \tilde{\mathbf{x}}\Vert_2\cos\theta $, $ \theta $ the angle
between $ \mathbf{w} / \Vert\mathbf{w}\Vert_2 $ and
$ \mathbf{x} - \tilde{\mathbf{x}} $, so $ |\mathbf{w}^\top(\mathbf{x} -
\tilde{\mathbf{x}})| / \Vert\mathbf{w}\Vert_2 $ gives the distance of
$ \mathbf{x} $ from the hyperplane that is $ \mathcal{B} $. The relationship
between $ \mathbf{w} $, $ \tilde{\mathbf{x}} $, $ \mathbf{x} $,
$ \mathcal{B} $ can be summarized visually with the below figure, providing
intuitive justification for our mathematics.
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[scale=1.2]
        % the decision boundary, with label
        \draw[gray, thick] (-3, -1) -- (3, 1);
        \node at (3.3, 1.1) {$ \mathcal{B} $};
        % w vector, with label (close to unit in length)
        \draw[blue, thick, -latex] (-2, -2/3) -- (-2.32, -2/3 + 0.96);
        \node[text=blue] at (-2.32, -2/3 + 1.29) {%
            $ \frac{\mathbf{w}}{\Vert\mathbf{w}\Vert_2} $%
        };
        % location of x prime, with label
        \filldraw[black] (0, 0) circle (2pt);
        \node at (0.3, -0.3) {$ \mathbf{x}' $};
        % location of x tilde, with label
        \filldraw[black] (-2, -2/3) circle (2pt);
        \node at (-1.7, -2/3 - 0.3) {$ \tilde{\mathbf{x}} $};
        % difference between x prime and x tilde
        \draw[black, thick, -latex] (-2, -2/3) -- (0, 0);
        % location of x, with label
        \filldraw[red] (-1, 1.2) circle (2pt);
        \node[text=red] at (-0.8, 1.5) {$ \mathbf{x} $};
        % difference between x and x tilde
        \draw[black, thick, -latex] (-2, -2/3) -- (-1, 1.2);
        % projection of x onto decision boundary, with label
        \draw[purple, thick, dashed] (-0.57, -0.19) -- (-1, 1.2);
        \node[text=purple] at (0, 0.8) {$ \frac{|\mathbf{w}^\top(\mathbf{x} -
        \tilde{\mathbf{x}})|}{\Vert\mathbf{w}\Vert_2} $};
    \end{tikzpicture}
    \label{fig:3.2.1}
\end{figure}

Noting that $ yf(\mathbf{x}) > 0 $ if $ f $ classifies $ \mathbf{x} $
correctly, $ yf(\mathbf{x}) < 0 $ if $ f $ classifies $ \mathbf{x} $
incorrectly, and that $ f(\tilde{\mathbf{x}}) = 0 $, as
$ \tilde{\mathbf{x}} \in \mathcal{B} $, so we can manipulate the
$ \mathbf{w}^\top(\mathbf{x} - \tilde{\mathbf{x}}) / \Vert\mathbf{w}\Vert_2 $
expression to see that
\begin{equation*}
    y\frac{
        \mathbf{w}^\top(\mathbf{x} - \tilde{\mathbf{x}})
    }{
        \Vert\mathbf{w}\Vert_2
    } =
    y\frac{
        \mathbf{w}^\top(\mathbf{x} - \tilde{\mathbf{x}}) + b - b
    }{
        \Vert\mathbf{w}\Vert_2
    } =
    y\frac{f(\mathbf{x}) - f(\tilde{\mathbf{x}})}{\Vert\mathbf{w}\Vert_2} =
    \frac{yf(\mathbf{x})}{\Vert\mathbf{w}\Vert_2}
\end{equation*}

We thus indeed see that as stated, $ yf(\mathbf{x}) / \Vert\mathbf{w}\Vert_2 $
is the signed distance of $ \mathbf{x} $ from the decision boundary
$ \mathcal{B} $ defined by $ f $, where
$ yf(\mathbf{x}) / \Vert\mathbf{w}\Vert_2 $ is positive if $ f $ classifies
$ \mathbf{x} $ correctly and negative if $ f $ classifies $ \mathbf{x} $
incorrectly.

\end{document}