\documentclass{article}

% standard article setup for worked exercises
\input{../utils/exercises_preamble}

\title{Numerical Optimization \\ \Large Worked Exercises}
\author{%
    Derek Huang\thanks{TD Securities, Quantitative Modeling and Analytics.}
}
\date{July 9, 2022}

\begin{document}

% define the \newtocsubection command
\input{../utils/newtocsubsection}

\maketitle

%\newpage

\tableofcontents

\newpage

\section{Introduction}

After developing an interest in optimization, I started looking for good textbooks I could use to feed this new interest and to serve as references.
One of which was Nocedal and Wright's \textit{Numerical Optimization}, which
I accidentally stumbled upon while reading the
\href{%
    https://docs.scipy.org/doc/scipy/reference/generated/%
    scipy.optimize.minimize.html%
}{documentation for \texttt{scipy.optimize.minimize}}, as I noticed that
several\footnote{%
    As of this writing, eight: BFGS, Newton-CG, dogleg, trust-ncg,
    trust-krylov, trust-exact.%
}
of the implemented minimization algorithms were from Nocedal and Wright's
book. That prompted my interest and led me to believe that Nocedal and
Wright's book would be very helpful for learning about and implementing the
algorithms that people use in production settings. I already had Boyd and
Vandenberghe's \textit{Convex Optimization} at the time, and although it is
excellent for understanding the theory, it was also only focused on convex
problems and most importantly, had very little discussion of real-life
algorithms. Therefore, getting the book was a no-brainer for me, given my
interests.

\medskip

% standardized closing transition
\input{../utils/intro_close}

\section{Line Search Methods}

\newtocsubsection{Exercise 3.1}

Although not required by the problem, we use a generic line search function
implemented via templates in C++. The type of line search method used is
simply specified via a direction search functor, which returns the search
direction from the current guess, and a step search functor, which returns the
step size from the current guess. For this exercise, we use a backtracking
line search implementing step search functor.

\medskip

TODO

\newtocsubsection{Exercise 3.3}

Let us define $ f : \mathbb{R}^d \rightarrow \mathbb{R} $ s.t. for
$ \mathbf{Q} \succeq m\mathbf{I} \in \mathbb{R}^{d \times d} $,
$ m \in (0, \infty) $, $ \mathbf{b} \in \mathbb{R}^d $,
\begin{equation} \label{eq:3.3.1}
    f(\mathbf{x}) \triangleq
    \frac{1}{2}\mathbf{x}^\top\mathbf{Qx} - \mathbf{b}^\top\mathbf{x}
\end{equation}

Note that $ \mathbf{Q} \succeq m\mathbf{I} \Rightarrow f $ is strongly convex.
Let us also define
$ f_{\mathbf{x}, \mathbf{p}} : \mathbb{R} \rightarrow \mathbb{R} $ for
$ \mathbf{x}, \mathbf{p} \in \mathbb{R}^d $ s.t.
\begin{equation} \label{eq:3.3.2}
    f_{\mathbf{x}, \mathbf{p}}(\alpha) \triangleq
    f(\mathbf{x} + \alpha\mathbf{p})
\end{equation}

Here $ \mathbf{p} \in \mathbb{R}^d $ is implicitly assumed to be a descent
direction, i.e. $ \mathbf{p}^\top\nabla f(\mathbf{x}) < 0 $, which is a fact
that will prove quite useful later. Regardless, we are interested in solving
the problem
\begin{equation} \label{eq:3.3.3}
    \begin{array}{ll}
        \displaystyle\min_\alpha & f_{\mathbf{x}, \mathbf{p}}(\alpha) \\
        \text{s.t.} & \alpha \in (0, \infty)
    \end{array}
\end{equation}

We see that (\ref{eq:3.3.3}) is the problem solved at every iteration when
using the exact line search, given that a descent direction $ \mathbf{p} $ has
already been computed. Expanding $ f_{\mathbf{x}, \mathbf{p}} $ as defined in
(\ref{eq:3.3.2}),
\begin{equation} \label{eq:3.3.4}
    \begin{split}
        f_{\mathbf{x}, \mathbf{p}}(\alpha) & =
        \frac{1}{2}(\mathbf{x} +
        \alpha\mathbf{p})^\top\mathbf{Q}(\mathbf{x} + \alpha\mathbf{p}) -
        \mathbf{b}^\top(\mathbf{x} + \alpha\mathbf{p}) \\
        & =
        \left(
            \frac{1}{2}\mathbf{x}^\top\mathbf{Q}\mathbf{x} -
            \mathbf{b}^\top\mathbf{x}
        \right) +
        \frac{1}{2}\alpha^2\mathbf{p}^\top\mathbf{Qp} +
        \alpha\mathbf{p}^\top(\mathbf{Qx} - \mathbf{b}) \\
        & =
        f(\mathbf{x}) + \frac{1}{2}\alpha^2\mathbf{p}^\top\mathbf{Qp} +
        \alpha\mathbf{p}^\top\nabla f(\mathbf{x})
    \end{split}
\end{equation}

From (\ref{eq:3.3.4}), since $ f(\mathbf{x}) $ is independent of $ \alpha $,
we see that instead of solving (\ref{eq:3.3.3}) we can solve
\begin{equation} \label{eq:3.3.5}
    \begin{array}{ll}
        \displaystyle\min_\alpha &
        \frac{1}{2}\alpha^2\mathbf{p}^\top\mathbf{Qp} +
        \alpha\mathbf{p}^\top\nabla f(\mathbf{x}) \\
        \text{s.t.} & \alpha \in (0, \infty)
    \end{array}
\end{equation}

Now, recall that $ \mathbf{p} $ is a descent direction, so
$ \mathbf{p}^\top\nabla f(\mathbf{x}) < 0 $. From (\ref{eq:3.3.4}), it's
clear that $ f'_{\mathbf{x}, \mathbf{p}}(0) =
\mathbf{p}^\top\nabla f(\mathbf{x}) < 0 $, so any minimizer $ \alpha^* $ of
(\ref{eq:3.3.5}) automatically satisfies the box constraint. Furthermore,
since $ \mathbf{Q} $ is positive definite and $ \mathbf{p} $ is necessarily
nonzero to be a descent direction, the objective of (\ref{eq:3.3.5}) is also
strongly convex, so $ \exists \eta \in (0, \infty) $ that is the unique
solution to (\ref{eq:3.3.5}). Taking the derivative of the objective at
$ \eta $, we see that
$ \eta\mathbf{p}^\top\mathbf{Qp} + \mathbf{p}^\top\nabla f(\mathbf{x}) = 0 $,
and so we can rearrange to get our desired result, which states that
\begin{equation} \label{eq:3.3.6}
    \eta =
    -\frac{\mathbf{p}^\top\nabla f(\mathbf{x})}{\mathbf{p}^\top\mathbf{Qp}}
\end{equation}

\newtocsubsection{Exercise 3.4}

Recalling the function $ f $ defined in (\ref{eq:3.3.1}) and the exact line
search result $ \eta $ defined in (\ref{eq:3.3.6}), we can show that $ \eta $
satisfies the Goldstein conditions. From (\ref{eq:3.3.4}) and
(\ref{eq:3.3.6}), noting that $ f_{\mathbf{x}, \mathbf{p}}(\eta) \triangleq
f(\mathbf{x} + \eta\mathbf{p}) $, we see that
\begin{equation} \label{eq:3.4.1}
    \begin{split}
        f(\mathbf{x} + \eta\mathbf{p}) & =
        f(\mathbf{x}) + \frac{1}{2}\eta^2\mathbf{p}^\top\mathbf{Qp} +
        \eta\mathbf{p}^\top\nabla f(\mathbf{x}) \\
        & =
        f(\mathbf{x}) + \frac{1}{2}\eta\mathbf{p}^\top\nabla f(\mathbf{x})
    \end{split}
\end{equation}

Here we use the fact that by (\ref{eq:3.3.6}),
$ \eta^2\mathbf{p}^\top\mathbf{Qp} =
-\eta\mathbf{p}^\top\nabla f(\mathbf{x}) $. Therefore, since
$ \mathbf{p}^\top\nabla f(\mathbf{x}) < 0 $,
$ \forall c \in \left(0, \frac{1}{2}\right) $,
\begin{equation} \label{eq:3.4.2}
    f(\mathbf{x}) + (1 - c)\eta\mathbf{p}^\top\nabla f(\mathbf{x}) \le
    f(\mathbf{x} + \eta\mathbf{p}) \le
    f(\mathbf{x}) + c\eta\mathbf{p}^\top\nabla f(\mathbf{x})
\end{equation}

We thus have our desired result in (\ref{eq:3.4.2}), which shows us that
$ \eta $ satisfies the Goldstein conditions. Furthermore, due to the result
in (\ref{eq:3.4.1}), it is also clear why we have
$ c \in \left(0, \frac{1}{2}\right) $.

\end{document}
