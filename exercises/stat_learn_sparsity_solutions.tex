\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsfonts, enumitem, fancyhdr, tikz}
% get rid of paragraph indent
\setlength{\parindent}{0 pt}
% allow section.equation numbering
\numberwithin{equation}{section}
\usepackage{hyperref}
% make the link colors blue, as well as cite colors. urls are magenta
\hypersetup{colorlinks, linkcolor=blue, citecolor=blue, urlcolor=magenta}

\title{Statistical Learning with Sparsity \\ \Large Exercise Solutions}
\author{Derek Huang\thanks{SMBC Capital Markets, Quantitative Strategists.}}
\date{March 20, 2022}

\begin{document}

\newcommand{\newtocsubsection}[1]{%
    \subsection*{#1} \addcontentsline{toc}{subsection}{#1}%
}

\maketitle

%\newpage

\tableofcontents

\newpage

\section{Introduction}

During my time as an undergraduate student at NYU, there was a period where
I found it interesting how the theory of convex optimization was applied by
the field of supervised learning to fit models. One of the topics I was
interested in was sparse optimization, mostly the kinds where sparsity was
enforced by a convex norm constraint, ex. $ \ell^1 $ norm for vectors,
nuclear norm for matrices.

\medskip

To satisfy my interests, I ended up purchasing Hastie, Tibshirani, and
Wainwright's \textit{%
    Statistical Learning with Sparsity: The Lasso and Generalizations%
}, seeing that it was a monograph on the topic. Despite enjoying the portions
of the book that I found time to read, I was irrationally afraid of
attempting the exercises, mostly because of a subconscious belief that failure
to satisfactorily complete meaningfully challenging exercises would somehow
invalidate both my intelligence and my worth as an individual. I no longer
feel the same way, although I still think that in general, my own intelligence
and individual worth are nothing significant.

\medskip

Regardless, the exercise solutions in this document exist to be a testament
to my own efforts towards understanding and will hopefully be a resource to
anyone else who may be attempting these exercises.

\section{The Lasso for Linear Models}

\newtocsubsection{Exercise 2.1} \label{sec:2.1}

Let $ \mathbf{x}_j  \in \mathbb{R}^N $ denote the $ j $-th column of the
predictor matrix $ \mathbf{X} \in \mathbb{R}^{N \times d} $, and let
$ \mathbf{y} \in \mathbb{R}^N $ denote the response vector as usual. Under
the assumptions of section 2.4, we have $ \frac{1}{N}\mathbf{1}^\top
\mathbf{y} = 0 $, $ \frac{1}{N}\mathbf{1}^\top\mathbf{x}_j = 0 $,
$ \frac{1}{N}\mathbf{x}_j^\top\mathbf{x}_j = 1 $\footnote{
    In words, the predictors are standardized to zero mean and unit variance,
    while the response is standardized to zero mean.
}.

\medskip

The lasso problem, as introduced in the book, can be written as
\begin{equation} \label{eq:std_lasso}
    \min_\mathbf{w}\frac{1}{2N}\Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2 +
    \lambda\Vert\mathbf{w}\Vert_1
\end{equation}

Let us replace $ \lambda \in (0, \infty) $ with $ \lambda_j \in
(0, \infty) $, where $ \forall j \in \{1, \ldots d\} $, $ \lambda_j
\Rightarrow w_{j'} \in \{1, \ldots d\}, j' \ne j, w_{j'} = 0 $.
With $ \lambda \triangleq \lambda_j $, only $ w_j $ may be nonzero, and
so in that case we can rewrite (\ref{eq:std_lasso}) where
$ \lambda \triangleq \lambda_j $ as
\begin{equation*}
    \min_{w_j}\frac{1}{2N}\sum_{k = 1}^N(y_k - x_{kj}w_j)^2 +
    \lambda_j|w_j|
\end{equation*}

Taking the subderivative of the above expression at its minimizer
$ \hat{w}_j $, we have
\begin{equation*}
    0 \in \left\{
        -\frac{1}{N}\sum_{k = 1}^Nx_{kj}(y_k - x_{kj}\hat{w}_j)
    \right\} +
    \lambda_j\partial|\hat{w}_j| =
    \left\{-\frac{1}{N}\mathbf{x}_j^\top\mathbf{y} + \hat{w}_j\right\} +
    \lambda_j\partial|\hat{w}_j|
\end{equation*}

Note that $ \hat{w}_j = 0 \Rightarrow \partial|\hat{w}_j| = [-1, 1] $. We
must then have $ \left|\frac{1}{N}\mathbf{x}_j^\top\mathbf{y}\right| \le
\lambda_j \Rightarrow \left|\frac{1}{N}\mathbf{x}_j^\top\mathbf{y}\right| $
is the smallest value of $ \lambda $ that guarantees $ \hat{w}_j = 0 $
conditional on $ \hat{w}_{j'} = 0 $, $ \forall j' \in \{1, \ldots d\} $,
$ j' \ne j $. Therefore, the smallest value of $ \lambda $,
which we denote $ \tilde{\lambda} $, that unconditionally guarantees
$ \hat{\mathbf{w}} = \mathbf{0} $, is such that
\begin{equation*}
    \tilde{\lambda} \triangleq
    \frac{1}{N}\max\big\{
        \big|\mathbf{x}_1^\top\mathbf{y}\big|, \ldots
        \big|\mathbf{x}_d^\top\mathbf{y}\big|
    \big\}
\end{equation*}

\newtocsubsection{Exercise 2.2}

The scalar soft-thresholding operator $ \mathcal{S}_\lambda $ presented in
section 2.4.1 is defined such that for $ \lambda \in (0, \infty) $,
\begin{equation} \label{eq:soft_threshold}
    \mathcal{S}_\lambda(x) \triangleq
    \operatorname{sgn}(x)\max\{0, |x| - \lambda\}
\end{equation}

For a scalar predictor, where we have an input vector $ \mathbf{x} \in
\mathbb{R}^N $, the lasso problem (\ref{eq:std_lasso}) simplifies to
\begin{equation} \label{eq:single_lasso}
    \min_w\frac{1}{2N}\sum_{k = 1}^N(y_k - x_kw)^2 + \lambda|w|
\end{equation}

We could solve for the minimizer $ \hat{w} \in \mathbb{R} $ of
(\ref{eq:single_lasso}) by taking a subderivative, but we are explicitly
told not to, so we instead proceed by considering the different cases of
$ \hat{w} $. Naturally, for simplicity, we first suppose that
$ \hat{w} \in (0, \infty) $. In that case, we can take a derivative of
(\ref{eq:single_lasso}) at $ \hat{w} $, and thus have
\begin{equation*}
    0 = -\frac{1}{N}\sum_{k = 1}^Nx_k(y_k - x_k\hat{w}) + \lambda =
    -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} + \lambda \Leftrightarrow
    \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} - \lambda
\end{equation*}

Here we use the fact that as in \nameref{sec:2.1}, the input vector
$ \mathbf{x} $ is given standardized, where $ \frac{1}{N}\mathbf{1}^\top
\mathbf{x} = 0 $, $ \frac{1}{N}\mathbf{x}^\top\mathbf{x} = 1 $, so
$ \frac{1}{N}\sum_{k = 1}^Nx_k^2\hat{w} $ simplifies to $ \hat{w} $. Now,
considering the case when $ \hat{w} \in (-\infty, 0) $, we have
\begin{equation*}
    0 = -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} - \lambda
    \Leftrightarrow \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} + \lambda
\end{equation*}

Noting that $ \lambda \in (0, \infty) $, we thus have $ \hat{w} \in
(0, \infty) \Leftrightarrow \frac{1}{N}\mathbf{x}^\top\mathbf{y} > \lambda $,
$ \hat{w} \in (-\infty, 0) \Leftrightarrow \frac{1}{N}\mathbf{x}^\top
\mathbf{y} < -\lambda $, which may be simplified into the condition that
$ \hat{w} \ne 0 \Leftrightarrow \left|\frac{1}{N}\mathbf{x}^\top
\mathbf{y}\right| > \lambda $. Therefore, by \textit{modus tollens} in both
directions, $ \hat{w} = 0 \Leftrightarrow \left|\frac{1}{N}\mathbf{x}^\top
\mathbf{y}\right| \le \lambda $. Furthermore, we can express $ \hat{w} $ in
terms of $ \frac{1}{N}\mathbf{x}^\top\mathbf{y} $ such that
\begin{equation*}
    \hat{w} =
    \operatorname{sgn}\left(\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right)
    \max\left\{
        \left|\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right| - \lambda, 0
    \right\} \triangleq
    \mathcal{S}_\lambda\left(\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right)
\end{equation*}

(\ref{eq:soft_threshold}) therefore yields the solution to
(\ref{eq:single_lasso}) when applied to
$ \frac{1}{N}\mathbf{x}^\top\mathbf{y} $.

\newtocsubsection{Exercise 2.3}

The subgradient, more precisely subderivative, equation for
(\ref{eq:single_lasso}) at a minimizer $ \hat{w} \in \mathbb{R} $ is
\begin{equation*}
    \begin{split}
        0 & \in
        \left\{-\frac{1}{N}\sum_{k = 1}^Nx_k(y_k - x_k\hat{w})\right\} +
        \lambda\partial|\hat{w}| \\
        0 & \in
        \left\{-\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w}\right\} +
        \lambda\partial|\hat{w}|
    \end{split}
\end{equation*}

Here we use the fact that $ \mathbf{x} \in \mathbb{R}^N $ is standardized
with $ \frac{1}{N}\mathbf{1}^\top\mathbf{x} = 0 $,
$ \frac{1}{N}\mathbf{x}^\top\mathbf{x} = 1 $. By the standard theory of
subgradients and considering the cases where $ \hat{w} \in (0, \infty) $,
$ \hat{w} \in(-\infty, 0) $, $ \hat{w} = 0 $, we see that
\begin{equation*}
    \begin{split}
            \hat{w} \in (0, \infty) & \Leftrightarrow
            0 \in \left\{
                -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} + \lambda
            \right\} \Leftrightarrow
            \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} - \lambda
            \Leftrightarrow \frac{1}{N}\mathbf{x}^\top\mathbf{y} > \lambda \\
            \hat{w} \in (-\infty, 0) & \Leftrightarrow
            0 \in \left\{
                -\frac{1}{N}\mathbf{x}^\top\mathbf{y} + \hat{w} - \lambda
            \right\} \Leftrightarrow
            \hat{w} = \frac{1}{N}\mathbf{x}^\top\mathbf{y} + \lambda
            \Leftrightarrow \frac{1}{N}\mathbf{x}^\top\mathbf{y} < -\lambda \\
            \hat{w} = 0 & \Leftrightarrow
            0 \in \left\{-\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right\} +
            \lambda[-1, 1] \Leftrightarrow
            \left|-\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right| \le \lambda
    \end{split}
\end{equation*}

We can write our results more neatly in the style of the book as
\begin{equation} \label{eq:2.3.1}
    \hat{w} = \left\{
        \begin{aligned}
            & \frac{1}{N}\mathbf{x}^\top\mathbf{y} - \lambda,
            & & \frac{1}{N}\mathbf{x}^\top\mathbf{y} > \lambda \\
            & 0, &
            & \frac{1}{N}\left|\mathbf{x}^\top\mathbf{y}\right| \le \lambda \\
            & \frac{1}{N}\mathbf{x}^\top\mathbf{y} + \lambda,
            & & \frac{1}{N}\mathbf{x}^\top\mathbf{y} < -\lambda
        \end{aligned}
    \right.
\end{equation}

Finally, from (\ref{eq:soft_threshold}), it's clear that we can express
(\ref{eq:2.3.1}) more succinctly as
\begin{equation*}
    \hat{w} =
    \mathcal{S}_\lambda\left(\frac{1}{N}\mathbf{x}^\top\mathbf{y}\right)
\end{equation*}

\newtocsubsection{Exercise 2.4}

TODO

\newtocsubsection{Exercise 2.7}

Let us consider the [random] response vector $ \mathbf{y} $, where
$ \mathbf{y}(\Omega) = \mathbb{R}^N $. Suppose we model $ \mathbf{y} $
using a non-adaptive linear model with independent, finite variance
additive errors. Then,
\begin{equation} \label{eq:std_linear_model}
    \mathbf{y} \triangleq \mathbf{Xw} + \varepsilon
\end{equation}

Here $ \mathbf{X} $ is the [random] input matrix, assumed
centered\footnote{
    We omit the intercept $ b $, since if the inputs are centered, for
    fixed $ \mathbf{y} $, $ \mathbf{X} $, its estimate $ \hat{b} \triangleq
    \frac{1}{N}\big(\mathbf{1}^\top\mathbf{y} -
    \mathbf{1}^\top\mathbf{X}\hat{\mathbf{w}}\big) $.
}, where $ \mathbf{X}(\Omega) = \mathbb{R}^{N \times d} $, $ \mathbf{w} \in
\mathbb{R}^d $ the true parameter vector, and $ \varepsilon $ the error
vector with i.i.d. components, where $ \forall k \in\{1, \ldots N\} $,
$ \mathbb{E}[\varepsilon_k] = 0 $, $ \operatorname{Var}(\varepsilon_k) =
\sigma^2 $. To fit (\ref{eq:std_linear_model}) by least squares, we want to
solve the convex problem
\begin{equation} \label{eq:std_linear_lsq}
    \min_\mathbf{w}\Vert\mathbf{y} - \mathbf{Xw}\Vert_2^2
\end{equation}

Let us assume that our $ d < N $ predictors are not collinear\footnote{
    If $ \operatorname{rank}(\mathbf{X}) \le \min\{N, d\} $, it is easy to
    see from our solution that the model's degrees of freedom will be
    $ \operatorname{rank}(\mathbf{X}) $.
}, i.e. $ \operatorname{rank}(\mathbf{X}) = d $. To simplify our analysis,
by [compact] singular value decomposition, we can write $ \mathbf{X} =
\mathbf{U\Sigma V}^\top $, where $ \mathbf{U} $, $ \mathbf{V} $ have $ d $
orthonormal columns and $ \mathbf{\Sigma} $ is $ d \times d $ with positive
diagonal. Since the minimizer $ \hat{\mathbf{w}} $ of
(\ref{eq:std_linear_lsq}) is such that $ \mathbf{X}^\top\mathbf{X}
\hat{\mathbf{w}} = \mathbf{X}^\top\mathbf{y} $, then
\begin{equation*}
    \mathbf{X}^\top\mathbf{X}\hat{\mathbf{w}} =
    \mathbf{V\Sigma}^2\mathbf{V}^\top\hat{\mathbf{w}} =
    \mathbf{V\Sigma U}^\top\mathbf{y} \Rightarrow
    \mathbf{V}^\top\hat{\mathbf{w}} =
    \mathbf{\Sigma}^{-1}\mathbf{U}^\top\mathbf{y}
\end{equation*}

Since the [random] predicted values $ \hat{\mathbf{y}} $ are such that
$ \hat{\mathbf{y}} \triangleq \mathbf{X}\hat{\mathbf{w}} $, we can use the
above result to write $ \hat{\mathbf{y}} $ in terms of $ \varepsilon $,
since with $ \mathbf{X} $ fixed $ \varepsilon $ becomes the only source of
randomness. We therefore see that
\begin{equation} \label{eq:2.7.1}
    \hat{\mathbf{y}} =
    \mathbf{U\Sigma}\big(\mathbf{V}^\top\hat{\mathbf{w}}\big) =
    \mathbf{UU}^\top\big(
        \mathbf{U\Sigma V}^\top\mathbf{w} + \varepsilon
    \big) = \mathbf{Xw} + \mathbf{UU}^\top\varepsilon
\end{equation}

As given in the text, the degrees of freedom $ \operatorname{df}(
\hat{\mathbf{y}}) $ of the specified linear model is such that
\begin{equation*}
    \operatorname{df}(\hat{\mathbf{y}}) \triangleq
    \frac{1}{\sigma^2}\sum_{k = 1}^N
    \operatorname{cov}(\hat{y}_k, y_k \mid \mathbf{X})
\end{equation*}

Since $ \operatorname{cov}(\hat{y}_k, y_k \mid \mathbf{X}) =
\mathbb{E}[\hat{y}_ky_k \mid \mathbf{X}] -
\mathbb{E}[\hat{y}_k \mid \mathbf{X}]\mathbb{E}[y_k \mid \mathbf{X}] $, by
linearity of expectation and by (\ref{eq:2.7.1}) we have
\begin{equation} \label{eq:2.7.2}
    \operatorname{df}(\hat{\mathbf{y}}) =
    \frac{1}{\sigma^2}
    \mathbb{E}\big[\hat{\mathbf{y}}^\top\mathbf{y} \ \big| \ \mathbf{X}\big] -
	\frac{1}{\sigma^2}\mathbb{E}[\hat{\mathbf{y}} \mid \mathbf{X}]^\top
    \mathbb{E}[\mathbf{y} \mid \mathbf{X}]
\end{equation}

Expanding the first and second terms of (\ref{eq:2.7.2}) and noting
$ \mathbb{E}[\varepsilon \mid \mathbf{X}] = \mathbf{0} $, we have from
(\ref{eq:std_linear_model}) and (\ref{eq:2.7.1})
\begin{equation*}
    \begin{split}
        \mathbb{E}\big[
            \hat{\mathbf{y}}^\top\mathbf{y} \ \big| \ \mathbf{X}
        \big] & =
        \mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} +
        \mathbf{w}^\top\mathbf{X}^\top
        \mathbb{E}[\varepsilon \mid \mathbf{X}] +
        \mathbb{E}[\varepsilon \mid \mathbf{X}]^\top
        \mathbf{UU}^\top\mathbf{Xw} +
        \mathbb{E}\big[
            \varepsilon^\top\mathbf{UU}^\top\varepsilon \ \big| \
            \mathbf{X}
        \big] \\ & =
        \mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw} +
        \mathbb{E}\big[
            \varepsilon^\top\mathbf{UU}^\top\varepsilon \ \big| \
            \mathbf{X}
        \big] \\
        \mathbb{E}[\hat{\mathbf{y}} \mid \mathbf{X}]^\top
        \mathbb{E}[\mathbf{y} \mid \mathbf{X}] & =
        \mathbf{w}^\top\mathbf{X}^\top\mathbf{Xw}
    \end{split}
\end{equation*}

We thus see that $ \operatorname{df}(\hat{\mathbf{y}}) = \frac{1}{\sigma^2}
\mathbb{E}\big[\varepsilon^\top\mathbf{UU}^\top\varepsilon \ \big| \
\mathbf{X}\big] $. Letting $ \mathbf{u}_j $ denote the $ j $th column of
$ \mathbf{U} $, then
\begin{equation*}
    \operatorname{df}(\hat{\mathbf{y}}) =
    \frac{1}{\sigma^2}\mathbb{E}\left[
        \begin{bmatrix}
            \ \varepsilon^\top\mathbf{u}_1 &
            \ldots &
            \varepsilon^\top\mathbf{u}_d \
        \end{bmatrix}
        \begin{bmatrix}
            \ \mathbf{u}_1^\top\varepsilon \ \\
            \ \vdots \ \\
            \ \mathbf{u}_d^\top\varepsilon \
        \end{bmatrix}
        \ \middle\vert \ \mathbf{X}
    \right] =
    \frac{1}{\sigma^2}\mathbb{E}\left[
        \sum_{j = 1}^d\big(\mathbf{u}_j^\top\varepsilon\big)^2
        \ \middle\vert \ \mathbf{X}
    \right] =
    \frac{1}{\sigma^2}\sum_{j = 1}^d\mathbb{E}\left[
        \big(\mathbf{u}_j^\top\varepsilon\big)^2
        \ \middle\vert \ \mathbf{X}
    \right]
\end{equation*}

Since $ \mathbb{E}\big[\mathbf{u}_j^\top\varepsilon \ \big| \
\mathbf{X}\big] = 0 $, then $ \mathbb{E}\left[\big(\mathbf{u}_j^\top
\varepsilon\big)^2 \ \middle\vert \ \mathbf{X}\right] = \operatorname{Var}
\big(\mathbf{u}_j^\top\varepsilon \ \big| \ \mathbf{X}\big) $. Recalling
that each $ \varepsilon_1, \ldots \varepsilon_N $ is i.i.d. with
$ \mathbb{E}[\varepsilon_k] = 0 $, $ \operatorname{Var}(\varepsilon_k) =
\sigma^2 $, $ \varepsilon \perp \mathbf{X} $, we see that
$ \operatorname{Var}\big(\mathbf{u}_j^\top\varepsilon \ \big| \
\mathbf{X}\big) = \sum_{k = 1}^Nu_{jk}^2\operatorname{Var}(\varepsilon_k) =
\sigma^2\mathbf{u}_j^\top\mathbf{u}_j = \sigma^2 $. Note that
$ \mathbf{u}_j^\top\mathbf{u}_j = 1 $ by the definition of the orthonormal
vector. Therefore, we see that
\begin{equation*}
    \operatorname{df}(\hat{\mathbf{y}}) =
    \frac{1}{\sigma^2}\sum_{j = 1}^d\operatorname{Var}\big(
        \mathbf{u}_j^\top\varepsilon \ \big| \ \mathbf{X}
    \big) = d
\end{equation*}

Therefore, a linear model fit by least squares using $ d $ non-collinear
predictors has exactly $ d $ degrees of freedom.

\end{document}